{"cells":[{"metadata":{"_uuid":"2c207e76bece57f6a99439c61229a1a68d4db600"},"cell_type":"markdown","source":"1. **Importing the libraries**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sea\n\n# Set a few plotting defaults\n%matplotlib inline\nplt.style.use('fivethirtyeight')\nplt.rcParams['font.size'] = 18\nplt.rcParams['patch.edgecolor'] = 'k'\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\n# Suppress warnings from pandas\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f2b0b82dcbe0197903a9d4f60668c64118822f22"},"cell_type":"markdown","source":"**2. Importing/exploring the train/test datasets and converting them to numeric form**"},{"metadata":{"trusted":true,"_uuid":"8de9709857437e6d20fc97d9ee8ea8e298dbcede"},"cell_type":"code","source":"\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/train.csv')\ntest_df = pd.read_csv('../input/test.csv')\nprint (\"Train Dataset: Rows, Columns: \", train_df.shape)\nprint (\"Test Dataset: Rows, Columns: \", test_df.shape)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ef51af0e7f03d5cc91d2d6deee18889df20efd2f"},"cell_type":"code","source":"train_valid = train_df.loc[train_df['parentesco1'] == 1, ['idhogar', 'Id', 'Target']].copy()\ntest_valid = test_df.loc[test_df['parentesco1'] == 1, ['idhogar', 'Id']].copy()\n\nsubmission_base = test_df[['Id', 'idhogar']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"00d13717877e6756b66160b0a2616c27eabfc28c"},"cell_type":"code","source":"#Glimpse at train_df\ntrain_df.head(7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"acd55c1cef69e2366fa7885094bcd81c2ede8910"},"cell_type":"code","source":"print (train_df.info())\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e66d6d716f1ff6d730804d304f481d7aab40505c"},"cell_type":"code","source":"print (\"Summary of Train Dataset: \")\ntrain_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e69f8013e8f3cc0944731569a705689ab74aeacc"},"cell_type":"code","source":"#select columns w/ dtype 'object'\ntrain_df.select_dtypes(['object']).head(15)\n#total of 5 columns\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c3b9bbf16b305d82dd4d0ff78f8fc9df30a74ab8"},"cell_type":"code","source":"#droping irrelevant nonumeric columns\ntrain_df = train_df.drop(['Id', 'idhogar'], axis = 1)\ntest_df = test_df.drop(['Id', 'idhogar'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0bbfbe85e22de74f84839b33c85eb72a24c907be"},"cell_type":"code","source":"#Let's explore the dependency column\ntrain_df['dependency'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b8968b2773fc2593c33c5623ebfe6959a98499f8"},"cell_type":"code","source":"#However, there is a column containing the square values if the dependency, 'SQBdependency'. \n#'yes' goes as 1 and 'no' goes as 0\n#Let's convert 'yes' and 'no' in dependency column to make it 100% numeric\ntrain_df['dependency'] = train_df['dependency'].replace(('yes', 'no'), (1, 0))\ntest_df['dependency'] = test_df['dependency'].replace(('yes', 'no'), (1, 0))\n#train_df['dependency']=train_df['dependency'].astype(float)\nprint (train_df['dependency'].unique())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a0d6240817d1799b3f7161f30fb1b5407521fe50"},"cell_type":"code","source":"train_df['edjefe'].unique()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"92e60fbf4c863d92c90802988ad54430dfd86386"},"cell_type":"code","source":"train_df['edjefa'].unique()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5a5e392e1cf9168bd00b47b8bb8e94a42aef1bee"},"cell_type":"code","source":"#let's convert 'no' to 0 and 'yes' to 1 to make the colums numeric\ntrain_df['edjefa'] = train_df['edjefa'].replace(('yes', 'no'), (1, 0))\ntrain_df['edjefe'] = train_df['edjefe'].replace(('yes', 'no'), (1, 0))\ntest_df['edjefa'] = test_df['edjefa'].replace(('yes', 'no'), (1, 0))\ntest_df['edjefe'] = test_df['edjefe'].replace(('yes', 'no'), (1, 0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8f20ac4e9a3dce9191a6f0a678d07e624b1f366f"},"cell_type":"code","source":"train_df['dependency']=train_df['dependency'].astype(float)\ntrain_df['edjefa']=train_df['edjefa'].astype(float)\ntrain_df['edjefe']=train_df['edjefe'].astype(float)\ntest_df['dependency']=test_df['dependency'].astype(float)\ntest_df['edjefa']=test_df['edjefa'].astype(float)\ntest_df['edjefe']=test_df['edjefe'].astype(float)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"04d7e0afd3e377616674d5ac30a3e92c9c932ed7"},"cell_type":"code","source":"#double checking that all columns are now numeric\ntrain_df.dtypes.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"105225123b5b2f727d1362e518bc9e6e54e5b0ae"},"cell_type":"markdown","source":"\n**3. Taking care of the missing values**"},{"metadata":{"trusted":true,"_uuid":"8530be791a45ac9bddc39261c33bfcd511f99f20"},"cell_type":"code","source":"#Now let's take care of the missing columns\nprint (\"Top Columns having missing values:\")\nmissing_df = train_df.isnull().sum().to_frame()\nmissing_df = missing_df.sort_values(0, ascending = False)\nmissing_df.head()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ad53a035a42b7bf86141673f9a9026519915f163"},"cell_type":"code","source":"#'v18q1' - number of tablets household owns\ntrain_df.groupby('v18q')['v18q1'].apply(lambda x: x.isnull().sum())\n#Every family that has nan for v18q1 does not own a tablet. \n#Therefore, we can fill in this missing value with zero.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8f0dd17a64f1b036e67a7256df79828c42467a31"},"cell_type":"code","source":"train_df['v18q1'] = train_df['v18q1'].fillna(0)\ntest_df['v18q1'] = test_df['v18q1'].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0c3947cb459026d9e265c0f81695059d31756f60"},"cell_type":"code","source":"#rez_esc - Years behind in school  \nprint(train_df['rez_esc'].value_counts())\nprint(train_df['instlevel6'].value_counts())\nprint (train_df.loc[train_df['rez_esc'].isnull()]['instlevel5'].value_counts())\nprint (train_df.loc[train_df['rez_esc'].isnull()]['instlevel6'].value_counts())\nprint (test_df.loc[test_df['rez_esc']==0]['instlevel6'].value_counts())\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0e9a4b5b903c4266510e14f20788bb0ae6c35b08"},"cell_type":"code","source":"#There is a good correlation between NA in 'Years behind in school'\n#and people with complete academic secondary level\n#Let's assign '6' to those people\ntrain_df['rez_esc'] = train_df['rez_esc'].fillna(6)\ntest_df['rez_esc'] = test_df['rez_esc'].fillna(6)\nprint(train_df['rez_esc'].value_counts())\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bcb4cb406b71872a922ba4bb8cb299ff76d992bf"},"cell_type":"code","source":"#v2a1, Monthly rent payment\nprint(train_df['v2a1'].unique())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7649ea2fccf0530cb9a3c5ee81f3dd36dffd5a98"},"cell_type":"code","source":"#Let's try to correlate it with 'tipovivi1, =1 own and fully paid house'\nprint(train_df['tipovivi3'].value_counts())\nprint (train_df.loc[train_df['v2a1'].isnull()]['tipovivi3'].value_counts())\n'''tipovivi1, =1 own and fully paid house, tipovivi2, \"=1 own,  paying in installments\", \ntipovivi3, =1 rented'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fc7e0abfc2d1a84d4c937a33e1f74f5db06a34de"},"cell_type":"code","source":"# - the column is about people not paying for rent - let's investigate it further\n#How many of those own a house?\nprint (train_df.loc[train_df['v2a1'].isnull()]['tipovivi1'].value_counts())\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1539b3f2afaadfdb8d48baef748db94af21c5d9b"},"cell_type":"code","source":"# so 6 out of 7 of 'na' people in that cathegory actually own and fully paid house.\n#Let's replace those na with the highest rent value to account for them possesing a house\na = max(train_df['v2a1'].max(), test_df['v2a1'].max())\nb = max(train_df['v2a1'].max(), test_df['v2a1'].max())\n# Fill in households that own the house with max rent payment (we'll assume they are the richest ones)\ntrain_df.loc[(train_df['tipovivi1'] == 1), 'v2a1'] = a\ntest_df.loc[(test_df['tipovivi1'] == 1), 'v2a1'] = a\n# Values for those with missing rent payment column will be replaced with min rent payment (we'll assume they are the poorest ones)\ntrain_df['v2a1'].fillna(b, inplace = True)\ntest_df['v2a1'].fillna(b, inplace = True)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d7344b80b707248497bbe7d506daef6da0cc6db6"},"cell_type":"code","source":"print (a)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9457a4ef21a6cebf1711e97d880d393f019e61e0"},"cell_type":"code","source":"#the rest of the missing values can be replaced with mean as their percentage towards total number of entries is insignificant\ntrain_df.fillna (train_df.mean(), inplace = True)\ntest_df.fillna(test_df.mean(), inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"861ada1e42b53afa995ada3af1d2d431a5ab207c"},"cell_type":"code","source":"print ('Columns having missing values:')\ntrain_df.columns[train_df.isnull().any()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6ea2ab4d9866de8019acaca16b0020090eb38850"},"cell_type":"code","source":"train_df['Target'].unique()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6ed8a34d958fe0c74d9b813cf0ab0e7b73d6c241"},"cell_type":"markdown","source":"**4. Dataset visuzalisation**"},{"metadata":{"trusted":true,"_uuid":"33f5354040af54699afdfdbf5f1cd794efffafbc"},"cell_type":"code","source":"print(train_df.shape[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a5683e562ae7c59c3eb7b68cadd2566f9e85a09a"},"cell_type":"code","source":"#Let's see how many unique value are in each of the columns:\ntrain_df.nunique().value_counts().sort_index().plot.bar(color = 'blue', figsize = (8, 6),\n                                                        edgecolor = 'k', linewidth = 2);\nplt.xlabel('Number of Unique Values'); plt.ylabel('Count');\nplt.title('Count of Unique Values in Integer Columns');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"88a6678e3951631b73e1615c925524b84ba2ea6b"},"cell_type":"code","source":"#let's separate binomial (yes-no, the majority) data from the rest\nnon_binomial = []\nfor i in range (0,train_df.shape[1]):\n    if len(train_df.iloc[:,i].unique().tolist())>2:\n        a = [i]\n        non_binomial = non_binomial + a\nprint (\"Non-Boolean Columns:\")\nprint (*non_binomial)\nfor i in non_binomial:\n    p = sea.countplot(data=train_df,x = train_df.iloc[:,i])\n    print (i)\n    plt.show()\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8a1dfe22271976ac02689cfc749cbbd4c6327b10"},"cell_type":"code","source":"import math\nfor i in non_binomial:\n    print (i)\n    p = sea.countplot(data=train_df,x = train_df.iloc[:,i])\n    plt.show()\n    '''train_df.iloc[:,i] = np.square(train_df.iloc[:,i])\n    test_df.iloc[:,i] = np.square(test_df.iloc[:,i])\n    p = sea.countplot(data=train_df,x = train_df.iloc[:,i])\n    plt.show()'''\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"63697c510e78b345a70e8f016ce15207ce8f6018"},"cell_type":"code","source":"# Dimension Reduction - dropping features with less than 2% correllation\ncorrelations = train_df.corr()['Target'].sort_values()\ncorrelations = correlations.reset_index().values\nprint('Original dataset shape: ',train_df.shape)\nfor column in range (0,len(correlations)-1):\n    if correlations[column,1]>-0.02 and correlations[column,1]<0.02 and correlations[column,0]!='SK_ID_CURR':\n        train_df=train_df.drop ([correlations[column,0]], axis=1)\nprint('Dataset shape after adjustments for correlation: ',train_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ef6a9a021de7765acabbf86bc91f7d04c7fb8327"},"cell_type":"code","source":"#removing the squared columns as redundant and thus introducing a bias:\nprint (train_df.shape)\ntrain_df = train_df.drop (['SQBescolari', 'SQBage', 'SQBhogar_total', 'SQBedjefe','SQBhogar_nin', 'SQBovercrowding','SQBdependency','SQBmeaned', 'agesq'], axis = 1)\nprint (train_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d5e48fa636e55260a20ccdae0ad90bd24b144dcb"},"cell_type":"code","source":"#realligning two datasets\ny_df = train_df['Target']\ntrain_df, test_df = train_df.align(test_df, join = 'inner', axis = 1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"195afc4fa3b7da7c9180ce65babf90b04bd0bd72"},"cell_type":"code","source":"print(f\"Training set shape:{train_df.shape}, testing set shape:{test_df.shape}\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3bf74181f84f64b2c3fc5f27def01f4cf5ca1e99"},"cell_type":"code","source":"print (train_df.dtypes.value_counts())\nprint (test_df.dtypes.value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"38d13dcc45b892f0b612c44a9381f8994585c028"},"cell_type":"code","source":"#converting to numpy array\nX = train_df.values\ny = y_df.values\ny = y.reshape(-1, 1)\nX.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"140ceb74f21ac4a1fc76a517eef046c513c68359"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split (X, y,test_size = 0.2, random_state = 0)\nX_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"59f407c43ad19eb6464a2502b378d53fbbaf3592"},"cell_type":"code","source":"\ny_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cb7e7bcbfd93beb2ff22ba4776d97f1f5a71f8f3"},"cell_type":"code","source":"np.unique(y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0cdb7424630385c92c79ae2724c7eaf859d2a1be"},"cell_type":"code","source":"X_train[0:10,:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8acd29fb35fe3d3b3db0000d9c141668f70b1016"},"cell_type":"code","source":"#Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\nX = sc.transform(X)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d8c1d385711a50f1372649e218e2358e03cb79e"},"cell_type":"code","source":"X_train[0:10,:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"321492dabb121d7480da11df6fb36617a7f5ad1f"},"cell_type":"code","source":"#tried using ANN\n'''#building ANN\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import GridSearchCV\ndef build_classifier(optimizer):\n    classifier = Sequential()\n    classifier.add(Dense(input_dim = 137, output_dim = 50, init = 'uniform', activation = 'softmax'))\n    classifier.add(Dense(output_dim = 100, init = 'uniform', activation = 'relu'))\n    classifier.add(Dense(output_dim = 100, init = 'uniform', activation = 'relu'))\n    classifier.add(Dense(output_dim = 100, init = 'uniform', activation = 'relu'))\n    classifier.add(Dense(output_dim = 5, init = 'uniform', activation = 'sigmoid'))\n    classifier.compile (optimizer = optimizer, loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n    return classifier\nclassifier = KerasClassifier(build_fn = build_classifier)\nparameters = {'batch_size': [15],\n              'epochs': [100, 150, 200, 250],\n              'optimizer': ['adam']}\ngrid_search = GridSearchCV(estimator = classifier,\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = 10)\ngrid_search = grid_search.fit(X_train, y_train)\nbest_parameters = grid_search.best_params_\nbest_accuracy = grid_search.best_score_\nprint ('Best accuracy: ',best_accuracy )\nprint ('Best parameters: ',best_parameters) # 'batch_size': 15, 'epochs': 250, 'optimizer': 'adam'\nclassifier = Sequential()\nclassifier.add(Dense(input_dim = test_df.shape[1], output_dim = 50, init = 'uniform', activation = 'tanh'))\nclassifier.add(Dense(output_dim = 100, init = 'uniform', activation = 'relu'))\nclassifier.add(Dense(output_dim = 100, init = 'uniform', activation = 'relu'))\nclassifier.add(Dense(output_dim = 100, init = 'uniform', activation = 'relu'))\nclassifier.add(Dense(output_dim = 100, init = 'uniform', activation = 'sigmoid'))\nclassifier.compile (optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n'''\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fdb80d84da7432163a14e9c4373fe4c001e45cb3"},"cell_type":"code","source":"#chose SVC\nfrom sklearn.svm import SVC\nclassifier = SVC(kernel = 'rbf', C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n    decision_function_shape='ovo', degree=3, gamma='scale',\n    max_iter=-1, probability=False, random_state=None, shrinking=True,\n    tol=0.001, verbose=False)\nclassifier.fit(X_train, y_train) #, batch_size = 15, epochs = 500\n#Below I expreimented a little bit with different parameter combinations\n#1597 tp for 100 epochs, 1624 tp for 150 epochs 1660 for 500, 1633 for 650, 1631 for 750, 1639 for 1000\n#1660 for 3 hidden layers, 1622 with 2 layers, 1645 for 4 layers\n#1660 for 100 hidden neurons per layer, 1660 for 200 hidden neurons, 1593 for 50 hidden neurons\n#1660 for sigmoid final layer, 0 for tanh!\n#1660 with softmax input activation, 1722,1710 with tanh, 1710 with additional 2nd tanh layer, 1694 with softmax as 2nd\n#1711 with tanh as middle layer (either 50 or 100 neurons)\n#1712 with 5 output neurons, 1695 with 1000\n#0.362 fro corr coef 2% (112), 1% - 0.363 (124) ,3% - 0.371, 4% - 0.381 (1735 with X_train), 5% - 1764, 6% - 1759, 1774/1749 with 7%, 1782/1759 with 8%, 1766-1788 for 10%, 1788 for -20%- 8%, 1786-1796 for 15%, 1766 with 20%\n#1788 tanh - sigmoid, 1774 sigmoid-sigmoid, 0 tanh-tanh, 1113 for sigmoid-tanh\n#0.381 with optimized ANN, 0.338 with kernel SVM \ntest_np = test_df.values\ny_pred = classifier.predict(X_test) #predict_classes for ANN\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5eab46eb5021727cac6d445379645ecc48d2f9df"},"cell_type":"code","source":"y_pred = y_pred.reshape(-1, 1)\ny_pred[0:100,0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a3367900d7a6b568b7882f1f9b232fbc884ec539"},"cell_type":"code","source":"# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nprint (cm)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"04b7f08a1eb8009a127025b7652b4ab9e6a32b94"},"cell_type":"code","source":"from sklearn.metrics import f1_score\nprint (f1_score(y_test, y_pred, average ='macro'))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"857c0808975d06b87a6314797a9db50aa3a6bc38"},"cell_type":"code","source":"test_np = sc.transform(test_np)\nclassifier.fit(X, y)\ny_pred = classifier.predict(test_np)#_classes\ny_pred = y_pred.reshape(-1, 1)\ny_pred[0:100,0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7606fa8969e3bfab5facd20d2cb9a3ce77635d0d"},"cell_type":"code","source":"test_df = pd.read_csv('../input/test.csv')\nsubmit = test_df[['Id']]\nsubmit['TARGET'] = y_pred\nsubmit.head()\n\n# Save the submission to a csv file\nsubmit.to_csv('SVMClassification.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5cd0ae1ce08e3c02dc3d71c8deb8ea608ef92a66"},"cell_type":"code","source":"submit.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"63028f46f5774f37729030b406a97060e4e59c75"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}